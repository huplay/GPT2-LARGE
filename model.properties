name = GPT-2 LARGE (774M)
source = OpenAI

# Number of different tokens
token.count = 50257

# END-OF-TEXT token id
end.of.text.token = 50256

# Maximum length of processed text (in tokens), which is the maximum value for position embedding
context.size = 1024

# Embedding size, which is the same as the hidden size
embedding.size = 1280

# Number of decoders
decoder.count = 36

# Number of attention heads
attention.head.count = 20

# Dividend at attention scoring (usually the square root of embeddingSize / headCount)
attention.score.dividend = 8

# Attention type for all decoders (global | local | none)
attention.type.1 = global
attention.type.2 = global
attention.type.3 = global
attention.type.4 = global
attention.type.5 = global
attention.type.6 = global
attention.type.7 = global
attention.type.8 = global
attention.type.9 = global
attention.type.10 = global
attention.type.11 = global
attention.type.12 = global
attention.type.13 = global
attention.type.14 = global
attention.type.15 = global
attention.type.16 = global
attention.type.17 = global
attention.type.18 = global
attention.type.19 = global
attention.type.20 = global
attention.type.21 = global
attention.type.22 = global
attention.type.23 = global
attention.type.24 = global
attention.type.25 = global
attention.type.26 = global
attention.type.27 = global
attention.type.28 = global
attention.type.29 = global
attention.type.30 = global
attention.type.31 = global
attention.type.32 = global
attention.type.33 = global
attention.type.34 = global
attention.type.35 = global
attention.type.36 = global

# Local attention size (necessary only if local attention type is used)
#attention.local.size = 128

# Epsilon, used at normalization
epsilon = 1e-5f